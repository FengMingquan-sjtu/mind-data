\PassOptionsToPackage{dvipsnames}{xcolor}
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\usepackage[colorlinks=true,urlcolor=black]{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{subcaption,ragged2e}

\usepackage[normalem]{ulem}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}
\usepackage{url}
\usepackage{tikz}
\usepackage{enumitem}
\usetikzlibrary{positioning}
\usepackage{array}
\usepackage{booktabs}
% \usepackage{graphicx, subcaption,ragged2e}
\usepackage{wrapfig}
\usepackage{caption} % For a better control over table captions
\usepackage{floatrow}
\usepackage{listings}
\usepackage{adjustbox}
\usepackage{footnote}
\usepackage{multirow}
\usepackage{amsmath}
% \usepackage[breaklinks=true]{hyperref}
\usepackage{breakcites}
\usepackage{blindtext}
\usepackage{svg}
\usepackage[most]{tcolorbox}
% \usepackage{cite}


% \DeclareUrlCommand\ULurl{%
%   \renewcommand\UrlFont{\ttfamily\color{blue}}%
%   \renewcommand\UrlLeft{\uline\bgroup}%
%   \renewcommand\UrlRight{\egroup}}

\DeclareUrlCommand\ULurl{%
  \renewcommand\UrlFont{\ttfamily\color{blue}}%
  \renewcommand\UrlLeft{}%
  \renewcommand\UrlRight{}}

\hypersetup{
    citecolor = {OliveGreen},
    linkcolor = {BrickRed}
}
\urlstyle{same}

\input{macros}



\title{MIND: Math Informed syNthetic Dialogues for Pretraining LLMs}



% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{Syeda Nahida Akter, Shrimai Prabhumoye, John Kamalu, Sanjeev Satheesh, Eric Nyberg, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }


\author{Syeda Nahida Akter$^{2}$\thanks{Work done during internship at NVIDIA},~~ Shrimai Prabhumoye$^{1,3}$,~~ John Kamalu$^{1}$,~~ Sanjeev Satheesh$^{1}$ \\\textbf{Eric Nyberg$^{2}$,~~ Mostofa Patwary$^{1}$,~~ Mohammad Shoeybi$^{1}$,~~ Bryan Catanzaro$^{1}$}\\
NVIDIA$^{1}$, Carnegie Mellon University$^{2}$, Boston University$^{3}$\\
\texttt{sakter@andrew.cmu.edu},~~\texttt{sprabhumoye@nvidia.com}}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
% \vspace{-5mm}
% \begin{center} \ULurl{https://research.nvidia.com/labs/adlr/Nemotron-MIND/}
% \end{center}

% \vspace{0.5em}



\begin{abstract}
The utility of synthetic data to enhance pretraining data quality and hence to improve downstream task accuracy has been widely explored in recent large language models ({\llm}s). Yet, these approaches fall inadequate %when assessing the performance of 
in complex, multi-hop and mathematical reasoning tasks as the synthetic data typically fails to add complementary knowledge to the existing raw corpus. In this work, we propose a novel large-scale and diverse \textbf{M}ath \textbf{I}nformed sy\textbf{N}thetic \textbf{D}ialogue (\ourapproach) generation method that improves the mathematical reasoning ability of {\llm}s. Specifically, using \ourapproach, we generate synthetic conversations based on \owm (\owma), resulting in a new math corpus, \ourdata. Our experiments with different conversational settings reveal that incorporating knowledge gaps between dialog participants is essential for generating high-quality math data. We further identify an effective way to format and integrate synthetic and raw data during pretraining to maximize the gain in mathematical reasoning, emphasizing the need to restructure raw data rather than use it as-is. Compared to pretraining just on raw data, a model pretrained on \ourdata shows significant boost in mathematical reasoning (\gsm: +13.42\%, \mathall: +2.30\%), including superior performance in specialized knowledge (\mmlu: +4.55\%, \mmlus: +4.28\%) and general purpose reasoning tasks (\textsc{General Reasoning}: +2.51\%). We release a \textbf{138B tokens} of synthetic conversations generated using \ourapproach at \ULurl{https://huggingface.co/datasets/nvidia/Nemotron-MIND}\footnote{We release the conversations generated from Nemotron4-340B-Instruct \citep{nvidia2024nemotron4340btechnicalreport} model and a detailed blog is available at \ULurl{https://research.nvidia.com/labs/adlr/Nemotron-MIND/}.}.

\end{abstract}


\input{sections/intro}
\input{sections/method}
\input{sections/experiments_revision}
\input{sections/ablations_revision}
\input{sections/related_works}

\section{Conclusion}
In this paper, we focus on improving the mathematical reasoning abilities of open-source {\llm}s. We propose a simple approach to generate complex and structured data at scale, called \ourapproach, that produces a new conversational synthetic math corpus, \ourdata, using an off-the-shelf open-source \llm. Models trained on \ourdata, a corpus generated through our approach, consistently outperform those trained on raw data, achieving up to a 6.29\% improvement across mathematical reasoning benchmarks and outperforming models trained on 3.6$\times$ larger datasets. Importantly, these gains persist across general-purpose reasoning tasks and when scaling up the data, highlighting the versatility of synthetic conversations. This work demonstrates the potential of structured conversational data to enhance reasoning, especially in cases where domain-specific high-quality data is limited, paving the way for more effective and resource-efficient pretraining of {\llm}s.



\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\input{sections/appendix}

\end{document}
