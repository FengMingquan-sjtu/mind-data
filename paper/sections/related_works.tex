\section{Related Works}

% \shrimai{We can move this section to before Conclusion}
\textbf{Mathematical Data Curation.} Selecting high quality data for pretraining {\llm}s is essential for producing state-of-the-art large language models \citep{NEURIPS2020_1457c0d6, chowdhery2023palm, parmar2024data, parmar2024nemotron, rae2021scaling, feng2024maximize}. Several mathematical datasets have been introduced in recent years \citep{paster2023openwebmath, wang2023mathpile, azerbayev2023proofnet, welleck2021naturalproofs} which have been carefully collected from the web using different heuristics. \owm contains 14.7B tokens of mathematical web pages filtered from \cc based on math strings, \LaTeX contents and a math document classifier. Building on this corpus, \dsm \citep{deepseek-math} trains a fastText \citep{joulin2016fasttext} classifier to further extract mathematical documents from \cc. They cluster the extracted documents based on the URL domain and label a domain math-related where over 10\% of the web pages have been collected are classified as math content. Finally, web pages linked to these URLs, yet uncollected, will be added to the seed corpus which will be used to retrain the fastText classifier to fetch diverse math contexts. \mathpile \citep{wang2023mathpile}, a multi-source corpus (8.9B tokens), has been aggregated from textbooks, Wikipedia, ProofWiki, \cc, StackExchange, and arXiv, with the majority (over 85\%) sourced from high quality data source arXiv. Although these datasets can effectively capture the diverse mathematical information from web, it is difficult to detect and filter out noisy dataset.
%and hence lowering the chances to obtain maximum gain from these corpora. 
Recently, many powerful models %, such as GPT-4 \citep{openai2024gpt4technicalreport}, Mistral \citep{jiang2023mistral7b}, Gemini \citep{geminiteam2024geminifamilyhighlycapable}, Claude \citep{anthropic}, Qwen \citep{qwen2.5} etc.
\citep{openai2024gpt4technicalreport, jiang2023mistral7b, geminiteam2024geminifamilyhighlycapable, anthropic, qwen2.5}, in addition to not open sourcing their data, are also refraining from disclosing detailed information about their corpus. % in their technical reports. 
For the open-source community, constructing high-quality and diverse pretraining corpora is a crucial factor in bridging the performance gap with closed-source models which is the main objective of our work. 

\textbf{Synthetic Math Data.}  Generating synthetic math data using {\llm}s has been widely explored in recent days \citep{53097, li2024common7blanguagemodels, gunasekar2023textbooksneed, madaan2024self, patel-etal-2024-datadreamer, toshniwal2024openmathinstruct118millionmath} specifically during alignment using supervised fine-tuning (SFT) \citep{alpaca}.  Some of the latest approaches focus on generating data from seed problems. For instance, \cite{yu2023metamath} rewrites existing benchmark questions from multiple perspectives using {\llm}s to create new mathematical problems, while \cite{huang2024keypointdrivendatasynthesisenhancement, shah2024aiassistedgenerationdifficultmath} leverage \gptllm to extract topics and key points from seed samples and recombine them into new questions. %\cite{shah2024aiassistedgenerationdifficultmath} follows a similar approach to create more challenging and diverse mathematical evaluation set. 
To further improve diversity, \cite{chan2024scalingsyntheticdatacreation} uses \gptllm to generate questions and answers at scale, incorporating over one million personas. %to ensure variety in the output. 
Previous approaches to generate synthetic data is primarily designed for fine-tuning rather than pretraining, distinguishing it from our effort. Similar to ours, \cite{dai2022dialoginpainting} converts documents into dialogues by predicting unobserved questions without altering the original document. However, \ourapproach expands knowledge by adding complementary reasoning and explanations, leveraging diverse conversational styles to enhance reasoning and enrich diversity, which is infeasible with \cite{dai2022dialoginpainting}. In the context of pretraining, recent works have generated synthetic datasets \citep{gunasekar2023textbooksneed,li2023textbooksneediiphi15} to train smaller language models that demonstrate equivalent performance as the larger models on certain mathematical benchmarks. However, these methods remain largely opaque, costly, and reliant on proprietary models to produce billions of tokens. Additionally, such data generation can be biased towards specifically generating data related to tasks that we want to perform well on. %Moreover, there has been limited exploration of strategies to maintain diversity, that is not limited to knowledge and intelligence of certain \llm, in synthetic data generation which is essential for pretraining. 
In contrast, \ourapproach provides a feasible alternative to upsample high quality structured data from diverse web contexts, that embeds multi-step and chain-of-thought reasoning, using an off-the-shelf open source \llm.
