\section{\ourapproach: Math Informed syNthetic Dialogue Generation}

\begin{figure}[H]
  \centering
  \includegraphics[width=\columnwidth]{figures/method_eq_name.pdf}
  \caption{\textbf{Math Informed syNthetic Dialogue.} We (a) manually design prompts of various conversational styles, (b) provide the prompt along with raw context as input to \llm to obtain diverse synthetic conversations, (c) apply heuristic filtering to refine the generated data and (d) observe the downstream task performance after continuously pretraining an 7B \llm.}%\shrimai{why is there a snowflake in the ``conversational generator"? We should remove this. It looks like the snowflake company logo. The figure looks much better now. I am wondering if there is a way to indicate that MIND is the left part of the figure and we evaluate the utility of MIND by doing the right part of the figure. I don't see any reference to this figure in the text. Please refer to this figure when you are describing the mathematical formulation and later in 3.1 and 3.2}\syeda{just to clarify `snowflake' here represents frozen LLM}}
  \label{fig:conv_math}
\end{figure}

% \shrimai{The heading says \ourapproach: Pretraining with Conversational Synthetic Math Data whereas everywhere else we write that it is a `conversational synthetic data generation approach'. I think we should keep it consistent.}
%Generating high-quality synthetic mathematical data at scale using off-the-shelf \llm is challenging and expensive due to the in-feasibility of verifying the quality of large scale mathematical contexts. %Unlike the synthetic SFT corpora which are built focusing on specific downstream tasks, it is difficult ensure diversity and scalability when it comes to generating pretraining corpora. 
%To address the data diversity and generation cost problems
\vspace{-3mm}
To generate high-quality data at scale, current synthetic data generation approach explores rephrasing texts using {\llm}s in varied syntax while preserving the core content \citep{rephrasing-the-web}. %However, this type of rephrasing limits up-sampling high-quality data in a way that does not go beyond grammatical styles or surface form transformations. %In addition, this approach can not inject new knowledge such as complimentary reasoning or explanations to the context which is important for complex problem solving. 
% In addition
However, their proposed approach limits up-sampling high-quality data in a way that does not go beyond grammatical styles or surface form transformations---leading little to no improvement when it comes to performance across complex and logical reasoning tasks. %(e.g., HellaSwag \citep{zellers2019hellaswag}, Winogrande \citep{sakaguchi2019winogrande}, \mmlu \citep{hendryckstest2021} etc.). 
We hypothesize that simple rephrasing does not leverage the full potential of the synthetic data to improve the mathematical and complex multi-hop reasoning ability of \llm. Therefore, we propose, \textbf{\ourapproach}, a conversational synthetic data generation approach that adds semantic variations and structured complexity to the raw text which is required to improve complex reasoning ability of the {\llm}s. In addition, multi-turn conversations can break down the original context step-by-step while each step addresses a sub-context at a time by often injecting complimentary reasoning or explanations. This resonates with how human solves a complex problem using consecutive chain-of-thought reasoning.

As depicted in \autoref{fig:conv_math}, given a raw dataset $\mathcal{R}=\{r_1, ...r_N\}$, we define a set of conversational prompts $\mathcal{P}=\{p_1, ...p_7\}$ and utilize a pretrained \llm, denoted as $\mathcal{M}$, for synthetic data generation.  We combine raw data $r_j$ with a prompt 
$p_i$ and pass it to $\mathcal{M}$ to produce synthetic conversation $s_{i,j}$.
\[
    s_{i,j} = \mathcal{M}(p_i \, || \, r_j)
\]
Here, \(s_{i,j}\) represents the synthetic data generated by applying prompt \(p_i\) to the raw example \(r_j\). For a specific prompt, the total synthetic data generated can be represented as 
% \[\mathcal{S} = \{s_{i,j} \mid i \in [1, 7], j \in [1, N]\}\] 
\[
\mathcal{S} = \{s_{i,j} \mid j \in [1, N]\} \text{ for a fixed } i \in [1, 7]
\]
We further apply heuristic filtering ($\mathcal{H}$) to remove bad generations:
\[
   \mathcal{S}' = \mathcal{H}(\mathcal{S})
\]

Finally, we have a high-quality synthetic dialogue corpus $\mathcal{S}'$ which is specifically designed to improve mathematical and logical reasoning ability. To summarize \ourapproach:
\[
   \mathcal{R} \rightarrow \text{\ourapproach} \rightarrow \mathcal{S}'
\]
To evaluate the effectiveness of $\mathcal{S}'$ in pretraining, we conduct continuous pretraining on a base \llm, $\mathcal{C}$, to minimize the computational costs associated with full pretraining. Our prior experiments on complete pretraining with raw data, $\mathcal{R}$ and synthetic data, $\mathcal{S}'$ validates that the ranking between models trained on $\mathcal{S}'$ or $\mathcal{R}$ remains consistent whether we use continuous pretraining or full-scale pretraining (detailed in Appendix \ref{ss:pt_results}). Moreover, continuous pretraining has emerged as an effective way  to improve performance of {\llm}s in target domains \citep{guo2024efficient, huang2023lawyer, chen2023meditron} and even boost their general capabilities \citep{ibrahim2024simple, parmar2024reusedontretrainrecipe} with reduced training cost. Given the similar outcomes and significant savings in computational resources, we adopt continued pretraining for evaluating our approach throughout the paper.

Using $\mathcal{S}'$ and a subset of pretraining data ($\mathcal{R}_{pt}$), the model $\mathcal{C}$ is continuously pretrained, yielding an enhanced model $\mathcal{E}$, which possess improved mathematical reasoning capabilities.
\[
   \mathcal{E} \leftarrow \text{pretrain}(\mathcal{C}, \mathcal{D}); ~\text{where} ~ \mathcal{D} = \{\mathcal{S}' \cup \mathcal{R}_{pt}\}
\]


% \shrimai{We need to add more details here: (1) this is not part of MIND, this is done to evaluate how good $ \mathcal{S}'$ is, (2) Why we pick this setup of continuous training? to save cost and resources. pretraining is costly and we have our old results of pretraining. We can put them in appendix and write here that our preliminary expts had shown that ranking between different methods does not change if we do CT vs complete pretraining. Alternatively, you can also find some citation for this. Give intuition about this setup here.}

% \shrimai{Mathematical formulation of your problem and pipeline. Given a raw data $\mathcal{R}$, we have a set of conversational prompts $p=\{p_1, ...p_7\}$ and a pretrained LLM used for generation of synthetic data $\mathcal{M}$. We take prompt $p_1$  and raw data and we generate synthetic data $s_1$. (mapping of raw data, prompt, model used for generation and synthetic data that you get out it.). Then we use $s_1$ in continued training setup of another model $\mathcal{C}$. At the end of that continued training you get model $\mathcal{E}$.}

% In this section, we introduce the detailed methodology of \ourapproach, as depicted in \autoref{fig:conv_math}. We first outline the data synthesis approach (Section \ref{ss:data_synthesis}). Next, we describe the training configuration for validating the significance of the synthetic corpus (Section \ref{ss:training_config}).

% \subsection{Conversational Data Synthesis} \label{ss:data_synthesis}

% Our conversational data synthesis approach comprises of following steps:

\subsection{Compose Conversational Prompts} 
To generate conversation using a document $r_i$, we prompt $\mathcal{M}$ in a way that preserves all information from the original context in the conversation and remains faithful to the context. 
We manually compose several prompts on diverse conversation setting and topics. We finalize seven prompts ($\mathcal{P}$) featuring conversations between (1) \tss, (2) \ts, (3) \tp, (4) \deb, (5) \ps, (6) \lk, and (7) \intr which can be found in Appendix \ref{ss:prompts}. These prompts are specifically designed to guide \llm in breaking down the input context step-by-step, with each step being discussed in depth through explanations and reasoning. %As a result, unlike simple rephrasing, the conversations generated using these prompts highlight decomposition of complex contexts into smaller contexts and include varied perspective about a single context coming from different participants.

%The selection criteria for the prompts are that they instruct the \llm to break down the input context step-by-step and discuss each step comprehensively by adding explanations and reasoning to it. 

% compose seven different prompts to instruct the \llm: (1) Two Students, (2) Teacher Student, (3) Two Professors, (4) Debate, (5) Problem Solving, (6) Layman Know-All, and (7) Interview. The key feature of each prompt is that, () it instructs the \llm to break down the input context step-by-step and discuss each step comprehensively by adding explanations and reasoning to it.


\subsection{Generate Conversation} 
Given an unstructured raw text ($r_{j}$), we instruct the \llm to convert the raw text into a multi-turn conversation ($s_{i,j}$) using a prompt ($p_{i}$) where $p_{i} \in ~\{\text{two\_students}, \text{teacher\_student}, ..., \text{debate}\}$.
% \begin{equation}
%     C_{p_j} = \llm(p_j \, || \, t_i)
%     \label{eq:conv_gen}
% \end{equation}


\textbf{Seed Data Selection.} The benefit of \ourapproach will get amplified for raw texts that require step-by-step analysis and chain of thought reasoning---the key features of a math corpus. Therefore, we choose \owm \citep{paster2023openwebmath} as our seed corpus, $\mathcal{R}$, which contains 14.7B tokens of high quality mathematical web text. %Therefore, we utilize raw texts from \owm to generate synthetic conversations at scale. \shrimai{combine 2 sent}


% \textbf{Large Language Model.} We use \llama \citep{llama3modelcard} as the \llm to generate conversation from raw text. \llamabase is an an auto-regressive language model with an optimized transformer architecture. The instruction tuned model is finetuned and optimized for for dialogue/chat use cases. The major requirements for the conversation generator are (1) to ensure no information loss between raw text and conversation and (2) to remain grounded to the raw text. Unlike smaller {\llm}s, the generated conversation from \llama  remains grounded to raw text and expands on it by adding reasoning and explanation in each turn of conversation.

\textbf{Large Language Model.} We use $\mathcal{M}=~$\llama \citep{llama3modelcard} to generate conversations from raw text, due to its superior performance across a variety of tasks compared to other open-source models. 
%\llamabase is an auto-regressive language model built on an optimized transformer architecture. 
The instruction-tuned version is specifically fine-tuned and optimized for dialogue and chat-based applications. 
% \shrimai{edit here}
%The key requirements for the conversation generator are (1) to prevent any loss of information between the raw text and the conversation, and (2) to ensure the conversation remains faithful to the original text. 
%Unlike smaller {\llm}s, \llama generated conversations stay closely grounded in the raw text, expanding on it with reasoning and explanation in each conversation turn.


\textbf{Generation Configuration.} We observe that with increasing context length, conversations tend to lose details from the original texts, as discussed in Appendix \ref{ss:context_len}. Therefore, for each generation, we iteratively take contexts of 500 tokens to obtain accurate and informative conversations. To evaluate the quality of the generated conversations, we test various filtering methods, from simple heuristics to \llm-based scoring. However, as noted in Appendix \ref{ss:quality_assess}, \llm scoring consistently rates all generations highly, making it unsuitable for our approach. Hence, we rely on heuristic filtering to discard bad generations before using them for training.
%Further details on context length and quality assessment of the conversation can be found in Section \ref{ss:context_len} and \ref{ss:quality_assess}.  

\section{Experimental Setup}

% \subsection{Generation Details} \label{ss:generation_config}

\paragraph{Conversation Generator Configuration.} To generate conversation, we consider zero-shot prompting $\mathcal{M}$, where we only pass a basic prompt (Appendix \ref{ss:prompts}) and the raw text. We sample conversations with \texttt{temperature=1.0} and \texttt{top\_p=0.9} where the total number of input-output tokes is limited to 4096. We use the TensorRT-LLM toolkit to deploy large scale generation\footnote{\url{https://github.com/NVIDIA/TensorRT-LLM}}. 

% \subsection{Architecture}

\paragraph{Pretrained Model Architecture.} We train a standard decoder-only Transformer \citep{NIPS2017_3f5ee243} architecture of 7B parameters ($\mathcal{C}$). The framework uses causal attention masks and Rotary Position Embeddings (RoPE) \citep{su2021roformer},  Tiktoken tokenizer, SwiGLU \citep{shazeer2020gluvariantsimprovetransformer} activations in the MLP layers, and grouped query attention (GQA) \citep{ainslie-etal-2023-gqa}. The model consists of 32 layers, 32 attention heads, sequence length of 4096, and a hidden dimension size of 4096. It has no bias terms, has dropout rate of zero, and untied input-output embeddings. The models are trained using NVIDIAâ€™s Megatron-LM \citep{shoeybi2019megatron} repository.


\subsection{Training Details}\label{ss:training_details}

\textbf{Pretraining Data.} Our pretraining data blend comprises of publicly available datasets from 13 snapshots of \cc (73.37\%) \citep{pile}, books/patents (9\%), papers (9\%), code (5.12\%), stack-exchange (2.66\%), and Wikipedia (0.8\%). Our code data consists of 42 programming languages while the other datasets come from various sources including web documents, news articles, scientific papers, and books. %We train the model on total 1T tokens of this data.

\textbf{General Pretraining.} To prepare a base model, we pretrain a 7B \llm on our pretraining data blend till 700B tokens using 512 H100 80GB SXM5 GPUs. During training, we use the AdamW optimizer \citep{loshchilov2018decoupled} with $\beta_1 =0.9$, $\beta_2=0.95$ and weight decay of 0.1. We use a 2-way tensor and pipeline parallelism to train the model. We set the maximum value of learning rate to $3e^{-4}$, minimum %learning rate 
to $3e^{-6}$, and use a batch size of 6M tokens with a 4096 context length.

% \shrimai{loss is pretraining loss.}

\textbf{Continued Pretraining.} After pretraining the base model ($\mathcal{C}$) on 700B tokens, we proceed with continuous pretraining using an additional 50B tokens to obtain $\mathcal{E}$. %\syeda{For our experiments, we create two baselines. The first baseline is formed by continuously training the base model on the pretraining blend till 50B tokens.} 
To reduce the shift between pretraining and continuous pretraining token distributions \citep{guo2024efficient} we %maintain consistency with the pretraining blend and 
create a new data blend ($\mathcal{D}$) for this phase.
To ensure the model is exposed to more math tokens, blend $\mathcal{D}$ consists of 2:1 ratio of \owm (33B tokens)---either raw ($\mathcal{R}$) or synthetic ($\mathcal{S}'$)--- and 13 snapshots of \cc (17B tokens) ($\mathcal{R}_{pt}$) to maintain consistency with the pretraining blend. %\shrimai{Aren't the same 13 CC used for pretraining upto 700B as well? Confirm and add the detail here}\syeda{No, I checked Dan's blend, the pretraining blend has all 13CC data with finewebedu score 0 to 5, here score 3,45 gets 1 epoch}. 
To ensure fair comparison, we always keep this token distribution constant in every experiment i.e., every model will see a the same amount of tokens from a data source regardless of its size. 
Unlike the pretraining blend, we use a high quality version of \cc data ($\mathcal{R}_{pt}$) filtered by the FineWebEdu \citep{penedo2024finewebdatasetsdecantingweb} classifier to achieve reasonable performance in generative tasks. This $\mathcal{R}_{pt}$ remains constant across all our continued pretraining experiments, while we vary the \owm %component by introducing synthetic data 
with $\mathcal{R}$ or $\mathcal{S}'$ or combining both to assess their relative significance. We maintain the same training configuration as before and continue pretraining until reaching 50B tokens, using the same pretraining loss objective. In this paper, we use two versions of \owm: 
\begin{itemize}[leftmargin=*]
\itemsep0em
\vspace{-2mm}
    \item \textbf{\owma-4B:} To quickly evaluate the effectiveness of all seven prompts, we take a smaller subset of \owm containing 4B tokens. Synthetic data generated from this subset is labeled as \ourdata-4B throughout the paper.
    \item \textbf{\owma-14B}: This version contains the full 14.7B tokens of \owm and the synthetic data of this is called \ourdata-14B.
\end{itemize}

\subsection{Evaluation Metrics}
% \subsection{Decay Experiments with data from 7 Conversational Styles}
To evaluate the zero-shot and few-shot learning capabilities of our models, we conduct a thorough benchmark assessment using a series of datasets using LM Eval Harness \citep{eval-harness}.

\textbf{General Purpose Reasoning Tasks.} This category comprises datasets testing broader cognitive skills and language comprehension. We consider nine standard commonsense and logical reasoning tasks in 0-shot: ARC easy (ARC-E) \& challenge (ARC-C) \citep{clark2018thinksolvedquestionanswering}, PIQA \citep{bisk2020piqa}, SIQA \citep{sap-etal-2019-social}, HellaSwag \citep{zellers2019hellaswag}, WinoGrande \citep{sakaguchi2021winogrande}, OpenBookQA \citep{OpenBookQA2018}, TruthfulQA \citep{lin-etal-2022-truthfulqa}, CommonsenseQA \citep{talmor-etal-2019-commonsenseqa} %. We further test the understanding and reasoning skills of the pretrained models on 
and a reading comprehension task: RACE \citep{lai-etal-2017-race}. We report the average results across ten general reasoning tasks under the metric `\textsc{General Reasoning}'.

\textbf{Math and Specialized Knowledge Tasks.} We consider three diverse math benchmarks to comprehensively evaluate the mathematical reasoning ability.
%using few-shot chain-of-thought prompting \citep{wei2022chain}. 
These benchmarks encompass mathematical challenges from elementary to college level complexity demanding qualitative reasoning (8-shot \gsm \citep{cobbe2021training}, 4-shot \mathall \citep{hendrycksmath2021}) and conceptual science and math reasoning %in multiple-choice question format 
(5-shot \mmlus \citep{hendryckstest2021}). In the Specialized Knowledge category, we evaluate on % include dataset that demand expertise in specific domains. 
\mmlu that spans multiple domains, from professional to academic, testing the model on specialized subjects.


% To quickly evaluate the effectiveness of different prompts, we begin with a subset of \owm containing 4B tokens, which we label as \owma-4B. The synthetic data generated from this subset is referred to as Syn-\owma-4B. For experiments involving the full 14.7B tokens of \owm, we label it as \owma, with the corresponding synthetic data termed Syn-\owma. 



% \subsection{}





