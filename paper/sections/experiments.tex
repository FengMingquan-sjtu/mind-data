\section{Experiments and Results}

% \shrimai{304 sents about token counts and how much data we have generate. highlighting that MIND has potential for vast amount of data generation from a limited amount of documents.}

% By employing the seven conversational prompts with the \owma-4B dataset, MIND generated a substantial corpus of 43 billion tokens. Furthermore, leveraging the entire \owma-14B dataset and incorporating two student conversation styles, MIND produced an additional 21 billion tokens. These results underscore MIND's potential for generating massive amounts of high-quality data from relatively limited source material.

By leveraging \ourapproach with seven conversational prompts and the raw \owma-4B, we generate a new corpus of 43 billion tokens (All Conversations). Additionally, employing the entire \owma-14B dataset and \tss conversation style, \ourapproach produces an additional 21 billion tokens---resulting in a total of 64 billion tokens. This underscores \ourapproach's potential to generate vast amount of high-quality data from relatively limited source material\footnote{To maintain consistency, we use a subset of the data (33B tokens) in all experiments.}.





% \subsection{Evaluation Results}

\textbf{Performance across Individual Prompt Style.} We observe the effect of each conversation style by generating synthetic data with seven prompts for a smaller subset of \owm, denoted as \owma-4B.  To establish a baseline, we continue pretraining $\mathcal{C}$ using the continuous pretraining blend, $\mathcal{D}=\{\mathcal{R}\cup\mathcal{R}_{pt}\}$, where $\mathcal{R}=\text{\owma-4B}$. In subsequent experiments, we replace $\mathcal{R}$ with $\mathcal{S}'$ where $\mathcal{S}'=\text{\ourdata-4B}$, corresponding to a particular conversation style, and repeat the training. To assess the utility of combining multiple conversations, we create a new dataset by selecting the longest conversation for each context from the seven generated conversations, labeling it as the \textsc{Longest Conversation} dataset.
% \shrimai{Everywhere following this section, we must refer to the data in a consistent manner with abstract etc. SYN-OWM and so on should be changed to MIND-OWM and so on.}

% For evaluation, we explicitly report performance on mathematical reasoning and specialized knowledge benchmarks, while presenting the average results across ten general reasoning tasks under the metric `\textsc{General Reasoning}'. Further breakdown of individual tasks are in Appendix \ref{ss:break_tasks}.

As shown in \autoref{tab:7b_owm_4b}, models trained on synthetic conversational data of individual styles consistently outperform the baseline across all reasoning tasks. Specifically, models trained on synthetic data exhibit significant improvements in mathematical reasoning compared to the baseline, achieving absolute gains ranging from 4.78\% to 12.82\% on \gsm, 0.54\% to 1.28\% on \mathall, and 0.79\% to 4.28\% on \mmlus. In specialized knowledge tasks such as \mmlu, synthetic data leads to improvements ranging from 1.08\% to 4.55\%. %compared to models trained on raw data. 
Furthermore, synthetic data yields an overall enhancement in general reasoning ability, with up to a 2\% absolute average improvement across the ten reasoning tasks. The \textsc{Longest Conversation} dataset delivers the highest gains across all tasks, demonstrating the potential of incorporating multiple perspectives into the training corpus.




\begin{table*}[h!]
\begin{center}
\resizebox{\textwidth}{!}{% 
\begin{tabular}{@{}lccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{Style} & \textbf{GSM8K} & \textbf{MATH}	& \textbf{\shortstack{MMLU-\\STEM}} & \textbf{\shortstack{MMLU}} & \textbf{\shortstack{\textsc{General Reasoning}\\(Avg)}}&\textbf{Avg-All}$^{*}$\\\toprule
\owma-4B &   Raw & 12.96	&4.92	&39.39	&45.91	&52.90&29.17\\ \midrule
% & \mawps           & \textbf{69.83}         & 66.83            \\ 
\multirow{7}{*}{\ourdata-4B} %& \tp & 13.50	&4.52	&37.93	&45.25	&53.21&29.12\\ 
 & \ts & 22.74	&5.96	&40.72	&47.93	&54.84&32.87\\ 
& \tss & 21.30	&6.20	&41.90	&48.77	&54.32&32.65\\  
& \lk & 17.74	&5.46	&41.96	&48.87	&54.89&31.74\\
& \deb & 23.96	&6.12	&40.18	&47.61	&54.76&33.11\\
& \intr & 20.92	&5.86	&40.53	&46.99	&54.73&32.12\\
& \ps & 24.72	&6.16	&41.36	&47.74	&\textbf{54.90}&33.38\\ \cmidrule{2-8}
& \textsc{Longest Conversation} & \textbf{25.78}	&\textbf{6.30}	&\textbf{42.72}	&\textbf{49.37}	&54.86&\textbf{34.08}\\
\toprule
\end{tabular}%
}
\end{center}
\caption[]{\textbf{Results of 7B \llm pretrained on Diverse Conversational Styles.} Continuous training with different conversation styles improves all reasoning tasks. Selecting the longest conversation for each raw text further enhances performance in 
math and specialized knowledge tasks\footnotemark. $^*$\textit{Average of \gsm, \mathall, \mmlu and General Reasoning}.}
\label{tab:7b_owm_4b}
\vspace{-2mm}
\end{table*}
\footnotetext{Further breakdown of individual tasks are in Appendix \ref{ss:break_tasks}.}

% \textbf{Analysis with Complete \owm.} Building on the findings from \owma-4B experiments, we confirm that all seven conversation prompts contribute to significant improvements compared to the raw data. Notably, the \tss prompt style emerged as one of the top three highest-performing prompts across all tasks. Unlike other high-performing styles such as \deb and \ps, which excel when the context involves a specific problem requiring a solution, the \tss style can be applicable to broader range of domains even when the context just simply describes concepts and knowledge. Given that \owma is approximately five times larger than \owma-4B with exposure of diverse reasoning domains, we choose the \tss prompt to generate synthetic conversations (Syn-\owma-14B) for the full \owm corpus, denoted as \owma-14B. 

\textbf{Analysis with Complete \owm.} Building on the findings from \owma-4B experiments, we establish that all seven conversational styles contribute to significant improvements compared to the raw data. This insight prompted us to explore the effect of increased data in reasoning by scaling our synthetic conversation generation for the complete \owma-14B corpus. To generate data, we follow the similar recipe as before and apply only one conversation style to minimize the generation cost. Among the top three highest-performing prompts across all tasks, we randomly choose \tss prompt style to generate conversations (\ourdata-14B). We %follow the same experimental setup as before, 
then continuously train $\mathcal{C}$ on \owma-14B and \ourdata-14B alternatively to assess the impact at a larger data scale. In this phase, we include another experiment by continuously training $\mathcal{C}$ on 50B additional tokens using $\mathcal{D}=\{\mathcal{R}_{pt}\}$ to observe how much gain we can attain across all tasks from math-centric pretraining. 




\begin{table*}[h!]
\begin{center}
\resizebox{\textwidth}{!}{% 
\begin{tabular}{@{}lccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{Style} & \textbf{GSM8K} &\textbf{MATH} & \textbf{\shortstack{MMLU-\\STEM}} & \textbf{MMLU}& 	\textbf{\shortstack{\textsc{General Reasoning}\\(Avg)}} & \textbf{Avg-All}\\\toprule
Pretraining Data & \multirow{2}{*}{Raw} & 9.33	& 4.74	& 37.84	& 45.41	& 53.22 & 28.17\\ 
\owma-14B & & 20.47 & 7.24 & 42.82 & 49.49 & 53.95& 32.79\\\midrule
\ourdata-14B & \tss & \textbf{27.29} & \textbf{8.24} & \textbf{43.55} & \textbf{49.91} & \textbf{55.54}& \textbf{35.25}\\ 
\toprule
\end{tabular}
}
\end{center}
\caption{\textbf{Results of 7B \llm trained on Complete \owma-14B and \ourdata-14B:} Continuous training of \llm with synthetic conversation outperforms models trained with original pretraining blend and raw \owm across all tasks. }
\label{tab:7b_owm}
\vspace{-2mm}
\end{table*}

As consistent with the previous findings, % from the \owma-4B experiments, 
\autoref{tab:7b_owm} shows that model trained on synthetic conversations is undoubtedly the best for math benchmarks while it also improves overall average for all other reasoning tasks. This underscores that, with data scaling, \ourapproach maintains significant gains in mathematical reasoning while preserving and enhancing performance across other reasoning tasks, including commonsense, factual, and specialized knowledge.


% \subsection{Continuous Training of Model with Larger Token Budget}


